<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Kafka学习笔记</title>
      <link href="/2020/03/12/Kafka/"/>
      <url>/2020/03/12/Kafka/</url>
      
        <content type="html"><![CDATA[<h2 id="1-关于Kafka"><a href="#1-关于Kafka" class="headerlink" title="1.关于Kafka"></a>1.关于Kafka</h2><h3 id="1-1名词解释"><a href="#1-1名词解释" class="headerlink" title="1.1名词解释"></a>1.1名词解释</h3><ul><li>Kafka是一种分布式的，基于发布/订阅的消息系统。</li><li>Broker(代理)：消息中间件处理节点，一个Kafka节点就是一个Broker，一个或多个Broker可以组成一个Kafka集群。</li><li>Topic(主题)：Kafka根据topic对消息进行归类，发布到Kafka的每条消息都要指定一个topic。</li><li>Partition(分区)：物理概念，一个topic可以分为多个Partition，每个Partition内部是有序的。</li><li>Replica(副本)：每个Partition有多个副本，存储在不同的broker上，保证消息的高可用。</li><li>Segment(片段)：Partition物理上由多个segment组成，每个segment存储这message信息。</li><li>Message(消息)：消息，是通讯的基本单位，由一个key，一个value和时间戳构成。</li><li>Producer(生产者)：消息生产者，向broker发送消息的客户端。</li><li>Consumer(消费者)：消息消费者，从broker读取消息的客户端。</li><li>ConsumerGroup(消费者群组)：每个consumer属于一个特定的consumer group，一条消息可以发送到多个不同的consumer group，但是一个consumer group中只能有一个consumer消费该消息。</li></ul><h3 id="1-2应用场景"><a href="#1-2应用场景" class="headerlink" title="1.2应用场景"></a>1.2应用场景</h3><ul><li>行为追踪：跟踪网站用户与前端应用发生的交互</li><li>传递消息：系统间异步的信息交互</li><li>日志收集：收集系统及应用程序的度量指标及日志</li><li>提交日志：将数据库的更新发布到Kafka上</li></ul><h2 id="2-Kafka生产者"><a href="#2-Kafka生产者" class="headerlink" title="2.Kafka生产者"></a>2.Kafka生产者</h2><img src="/2020/03/12/Kafka/Kafka-Producer.png" alt="kafka-producer" style="zoom: 80%;"><p>​        send()方法大致过程为：设置序列化器-&gt;设置分区-&gt;放入队列缓存-&gt;等待时机push到broker。</p><h3 id="2-1生产者选择分区"><a href="#2-1生产者选择分区" class="headerlink" title="2.1生产者选择分区"></a>2.1生产者选择分区</h3><p><img src="/2020/03/12/Kafka/%E9%80%89%E6%8B%A9%E5%88%86%E5%8C%BA.png" alt="选择分区流程"></p><h3 id="2-2序列化器和发送"><a href="#2-2序列化器和发送" class="headerlink" title="2.2序列化器和发送"></a>2.2序列化器和发送</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Future&lt;RecordMetadata&gt; <span class="title">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span></span>;</span><br><span class="line"><span class="function">Future&lt;RecordMetadata&gt; <span class="title">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span></span>;</span><br></pre></td></tr></table></figure><p>两个方法其实都是异步返回的。</p><p>同步方式：第一种，调用send()后，马上get()，实现同步调用；</p><p>异步方式：第二种，在callback中进行内容处理，实现异步调用。</p><p><strong>序列化器</strong></p><ul><li><p>内置序列化器(int/long/float/double/byte/string)</p><p>一般将message组织成标准的json字符串进行传递</p></li><li><p>自定义序列化器(实现org.apache.kafka.common.serialization.Serializer接口)</p></li></ul><h3 id="2-3生产者配置"><a href="#2-3生产者配置" class="headerlink" title="2.3生产者配置"></a>2.3生产者配置</h3><ul><li><strong>acks</strong> [0,1,all]，三个值分别对应立即返回响应/leader broker写入成功响应/所有相关broker写入成功响应</li><li>buffer.memory 生产者内缓存区域的大小，生产者用它缓存要发送到服务器的消息</li><li>compression.type 消息发送时的压缩类型，默认不会压缩，可以设置为snappy、gzip或lz4</li><li>retries 重发消息次数</li><li>batch.size 发送到同一个Partition的消息会被先存储在batch中，该参数指定batch大小，单位byte。不一定需要等到batch被填满才能发送</li><li>linger.ms 在发送消息前等待linger.ms，从而等待更多消息加入到batch中。消息发送的时机：<strong>batch被填满或linger.ms达到上限</strong></li><li>max.in.flight.requests.per.connection 生产者在收到服务器响应之前可以发送消息的个数。设置为1可以保证顺序</li></ul><h2 id="3-Kafka消费者"><a href="#3-Kafka消费者" class="headerlink" title="3.Kafka消费者"></a>3.Kafka消费者</h2><h3 id="3-1消费模式"><a href="#3-1消费模式" class="headerlink" title="3.1消费模式"></a>3.1消费模式</h3><p><img src="/2020/03/12/Kafka/%E6%B6%88%E8%B4%B9%E6%A8%A1%E5%BC%8F.png" alt="消费模式"></p><p>说明：</p><ol><li>⼀一个主题可以被多个组消费</li><li>同一组中不同的消费者消费的分区一定不会重复，所有的消费者一起消费所有分区</li><li>主题可以被一个组反复消费，只要消息没有被删除</li><li>再均衡：分区的所有权从一个消费者转移到另一个消费者</li></ol><h3 id="3-2消费过程"><a href="#3-2消费过程" class="headerlink" title="3.2消费过程"></a>3.2消费过程</h3><p><img src="/2020/03/12/Kafka/%E6%B6%88%E8%B4%B9%E8%BF%87%E7%A8%8B.png" alt="消费过程"></p><p><img src="/2020/03/12/Kafka/%E6%8F%90%E4%BA%A4%E5%81%8F%E7%A7%BB%E9%87%8F.png" alt="提交偏移量"></p><h2 id="4-Broker"><a href="#4-Broker" class="headerlink" title="4.Broker"></a>4.Broker</h2><p><img src="/2020/03/12/Kafka/%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E5%85%B3%E7%B3%BB.png" alt="集群成员关系"></p><p>broker即Kafka集群中的一个节点，上面保存着partition（Leader）及partition副本（Follower），其中leader对外提供数据的读写服务，只有当leader挂掉之后，follower才可以通过竞选产生新的leader提供服务。follower与leader之间的同步是follower去leader处pull的过程。生产者发送消息到broker后，会根据配置的acks的值，来决定何时返回响应。</p><p>controller：</p><p>Kafka集群启动时各broker向zookeeper中/controller注册临时节点，首先注册成功的节点成为controller，controller除具有一般的broker功能之外，还负责分区leader的选举，监听zk变化并同步到其他的broker</p><p><strong>分区复制</strong></p><ul><li>leader副本：每个分区都有一个leader副本，它负责处理所有生产者和消费者的请求</li><li>follower副本：leader之外的其他副本，它不处理来自客户端的请求，唯一的任务就是从leader处同步消息，保持与leader一致</li><li>AR：Assigned Replicas，所有副本</li><li>ISR：In-Sync Replicas，已同步副本</li><li>OSR：Out-of-Sync Replicas，掉队的副本</li><li>AR = ISR + OSR</li></ul><h2 id="5-元数据管理"><a href="#5-元数据管理" class="headerlink" title="5.元数据管理"></a>5.元数据管理</h2><h3 id="5-1ZK结构树"><a href="#5-1ZK结构树" class="headerlink" title="5.1ZK结构树"></a>5.1ZK结构树</h3><p><img src="/2020/03/12/Kafka/ZK%E7%BB%93%E6%9E%84%E6%A0%91.png" alt="zk结构树"></p><p>可以在server.properties中配置zookeeper.connect=ip:port/kafka来设置顶层路径，否则默认在根路径下</p><h3 id="5-2brokers节点"><a href="#5-2brokers节点" class="headerlink" title="5.2brokers节点"></a>5.2brokers节点</h3><p><img src="/2020/03/12/Kafka/brokers%E8%8A%82%E7%82%B9.png" alt="brokers节点.png"></p><h3 id="5-3controller节点"><a href="#5-3controller节点" class="headerlink" title="5.3controller节点"></a>5.3controller节点</h3><p><img src="/2020/03/12/Kafka/controller%E8%8A%82%E7%82%B9.png" alt="controller节点.png"></p><h2 id="6-Kafka物理存储"><a href="#6-Kafka物理存储" class="headerlink" title="6.Kafka物理存储"></a>6.Kafka物理存储</h2><p><img src="/2020/03/12/Kafka/%E7%89%A9%E7%90%86%E5%AD%98%E5%82%A8.png" alt="物理存储.png"></p><p>producer发送的消息落到broker的内存，然后定时写到磁盘，当文件逐渐变大时会切分成小的文件（segment），而这些文件会根据配置的保存时间在超期后删除。</p><h3 id="6-1刷盘"><a href="#6-1刷盘" class="headerlink" title="6.1刷盘"></a>6.1刷盘</h3><p>log.flush.interval.messages：在数据被写入到硬盘前最大积累的消息数量</p><p>log.flush.interval.ms：在数据被写入到硬盘前最大时间</p><p>log.flush.scheduler.interval.ms：检查数据是否要写入到硬盘的时间间隔</p><h3 id="6-2文件滚动"><a href="#6-2文件滚动" class="headerlink" title="6.2文件滚动"></a>6.2文件滚动</h3><p>默认情况下，每个segment包含1G或者7天的数据，以较小值为准，如果达到片段上限，就会关闭当前文件，并打开一个新的文件。</p><h3 id="6-3数据清除策略"><a href="#6-3数据清除策略" class="headerlink" title="6.3数据清除策略"></a>6.3数据清除策略</h3><p>log.cleanup.policy：日志清除策略（delete/compact）</p><p>log.retention.bytes：日志保留量</p><p>log.retention.hours：日志保留时间</p><p><img src="/2020/03/12/Kafka/compact%E5%8E%BB%E9%87%8D.png" alt="compact去重.png"></p><p>一般都会使用delete策略</p><h3 id="6-4文件读取"><a href="#6-4文件读取" class="headerlink" title="6.4文件读取"></a>6.4文件读取</h3><p><img src="/2020/03/12/Kafka/%E6%96%87%E4%BB%B6%E7%B4%A2%E5%BC%95%E5%AF%B9%E5%BA%94.png" alt="文件索引对应.png"></p><ul><li>定位message<ol><li>二分查找index file</li><li>通过index file定位物理位置</li><li>顺序扫描log file</li></ol></li><li>稀疏索引</li></ul><h3 id="6-5关于-consumer-offsets"><a href="#6-5关于-consumer-offsets" class="headerlink" title="6.5关于_consumer_offsets"></a>6.5关于_consumer_offsets</h3><ul><li><p>分区方式：</p><p>默认50个分区，按group.id来分区：Math.abs(groupId.hashCode()) % 50</p></li><li><p>日志格式：存放3类数据，格式不同</p><p>group元数据：Group::GroupMetadata(groupId,members)</p><p>offset信息：[Group,Topic,Partition]::[OffsetMetadata[Offset,Metadata],CommitTime,ExpirationTime]</p><p>tombstone消息：删除标记，只有key，value为空的消息</p></li><li><p>留存策略</p><p>通过compact保留每个groupId的最新数据，如group下没有任何active成员且所有的位移数据都已被删除时，kafka也会定时清除group的元数据</p></li></ul><h2 id="7-可靠性"><a href="#7-可靠性" class="headerlink" title="7.可靠性"></a>7.可靠性</h2><p>复制机制和分区的多副本架构是Kafka可靠性保证的核心</p><ol><li>单个分区内的消息是有序的</li><li>只有当消息被写入分区的所有同步副本（ISR）时，它才被认为是“已提交”的</li><li>消费者只读“已提交”的消息</li><li>只要有一个副本活跃，“已提交”的消息就不会丢失</li></ol><h3 id="7-1broker可靠性"><a href="#7-1broker可靠性" class="headerlink" title="7.1broker可靠性"></a>7.1broker可靠性</h3><ul><li><p>副本系数：replication.factor = 3</p></li><li><p>不完全的选举：unclean.leader.election.enable = true/false</p><p>允许不同步的副本成为leader，可用性和一致性之间的权衡</p></li><li><p>最小同步副本：min.insync.replicas = 2</p><p>同步副本小于min.insync.replicas，会阻止生产者继续写入消息（NotEnoughReplicasException）</p></li></ul><h3 id="7-2生产者可靠性"><a href="#7-2生产者可靠性" class="headerlink" title="7.2生产者可靠性"></a>7.2生产者可靠性</h3><ul><li><p>发送确认：acks = 0/1/all</p><p>0代表生产者能够通过网络把消息发送出去</p><p>1代表leader副本已经收到消息并把它写入到分区文件</p><p>all代表所有副本都收到消息</p></li><li><p>失败重试：retries = 3</p><p>可用于解决可重试的错误，如LEADER_NOT_AVAILABLE、网络异常等</p></li><li><p>额外的异常处理：无法通过自动重试解决的问题</p><p>可用于解决不可重试的错误：如消息大小错误、认证错误、序列化错误，或者重试达到上限</p></li></ul><h3 id="7-3消费者可靠性"><a href="#7-3消费者可靠性" class="headerlink" title="7.3消费者可靠性"></a>7.3消费者可靠性</h3><ul><li><p>group.id = &lt;集群内唯一&gt;</p><p>不同的消费者配置不同的group.id，确保各自能消费topic的全量数据</p></li><li><p>auto.offset.reset = earliest / latest</p><p>在没有偏移量提交时或请求的偏移量在broker不存在时，启用此配置</p></li><li><p>enable.auto.commit = true</p></li><li><p>auto.commit.interval.ms = 300000</p><p>自动提交的频率，默认是5秒钟提交一次</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> MQ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>springboot总结(一)</title>
      <link href="/2020/03/09/springboot/"/>
      <url>/2020/03/09/springboot/</url>
      
        <content type="html"><![CDATA[<blockquote><p>开始使用SpringBoot时是看了技术大佬–<a href="http://www.ityouknow.com/" target="_blank" rel="noopener">纯洁的微笑</a>的博客，内容全面，讲解清晰，经常当做工具书来查看，想要学习SpringBoot的小伙伴可以移步到<a href="http://www.ityouknow.com/" target="_blank" rel="noopener">他的博客</a>。</p><p>这次总结也是翻看着大佬的博客，想要整理出一些东西来，算是给自己系统性的加深一下记忆吧。</p></blockquote><a id="more"></a><h2 id="1-谈谈SpringBoot"><a href="#1-谈谈SpringBoot" class="headerlink" title="1.谈谈SpringBoot"></a>1.谈谈SpringBoot</h2><p>​        说到SpringBoot，首先想到的是它<strong>约定优于配置（convention over configuration）</strong>的设计理念，这让我们从大量的样板化的配置文件中解脱出来，使开发人员只需提供最简单的配置甚至不需要配置，引入<strong>Starter</strong>，我们便<strong>开箱即用</strong>，这种简洁是多么幸福的一件事。</p><blockquote><p>参考<a href="http://www.ityouknow.com/springboot/2019/06/03/spring-boot-hot.html" target="_blank" rel="noopener">Spring Boot 为什么这么火？</a></p></blockquote><h2 id="2-SpringBoot的自动配置原理"><a href="#2-SpringBoot的自动配置原理" class="headerlink" title="2.SpringBoot的自动配置原理"></a>2.SpringBoot的自动配置原理</h2><p>​        SpringBoot项目的启动注解是<strong>@SpringBootApplication</strong>，它是由@Configuration，@ComponentScan，@EnableAutoConfiguration三个注解组成，其中<strong>@EnableAutoConfiguration</strong>是实现自动配置的入口，该注解通过@Import注解导入了<strong>AutoConfigurationImportSelector</strong>，并在该类中加载META-INF/spring.factories的配置信息，然后筛选出以EnableAutoConfiguration为key的数据，加载到IOC容器中，从而实现自动配置功能。</p><p>参考<a href="http://www.ityouknow.com/springboot/2019/07/24/springboot-interview.html" target="_blank" rel="noopener">Spring Boot 面试，一个问题就干趴下了！</a></p><h2 id="3-我用过的一些"><a href="#3-我用过的一些" class="headerlink" title="3.我用过的一些"></a>3.我用过的一些</h2><h4 id="3-1mybatis-spring-boot-starter"><a href="#3-1mybatis-spring-boot-starter" class="headerlink" title="3.1mybatis-spring-boot-starter"></a>3.1mybatis-spring-boot-starter</h4><p>参考<a href="http://www.ityouknow.com/springboot/2016/11/06/spring-boot-mybatis.html" target="_blank" rel="noopener">如何优雅的使用 Mybatis</a></p><p>​        SpringBoot把一切都化繁为简，使用mybatis-spring-boot-starter首先需要在pom中引入依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis.spring.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>&#123;mybatis.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>mybatis-spring-boot-starter</code>主要有两种解决方案，一种是使用注解解决一切问题，通过@Insert、@Delete、@Update、@Select注解将SQL绑定到Mapper接口的方法，当实体类属性和数据库字段名不一致时，可以通过@Result来返回结果的字段映射；另一种是简化后的老传统，需要我们去编写mapper.xml文件，通过Mapper接口中的方法名来映射到对应的SQL语句。我个人更偏向于使用第二种方案。下面对第二种方案进行介绍：</p><ol><li><p>引入maven依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis.spring.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>&#123;mybatis.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--数据库驱动--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>application.properties 添加相关配置</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">spring.datasource.url</span>=<span class="string">jdbc:mysql://localhost:3306/test?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=true</span></span><br><span class="line"><span class="meta">spring.datasource.username</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">spring.datasource.password</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">spring.datasource.driver-class-name</span>=<span class="string">com.mysql.cj.jdbc.Driver</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#mybatis.config-location=classpath:mybatis-config.xml</span></span><br><span class="line"><span class="meta">mybatis.mapper-locations</span>=<span class="string">classpath:mapper/*.xml</span></span><br></pre></td></tr></table></figure></li><li><p>编写Mapper接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">UserMapper</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function">List&lt;UserEntity&gt; <span class="title">getAll</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">UserEntity <span class="title">getOne</span><span class="params">(Long id)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(UserEntity user)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">update</span><span class="params">(UserEntity user)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">delete</span><span class="params">(Long id)</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>添加映射文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">"com.xxx.mapper.UserMapper"</span> &gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">resultMap</span> <span class="attr">id</span>=<span class="string">"BaseResultMap"</span> <span class="attr">type</span>=<span class="string">"com.xxx.entity.UserEntity"</span> &gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span> <span class="attr">column</span>=<span class="string">"id"</span> <span class="attr">property</span>=<span class="string">"id"</span> <span class="attr">jdbcType</span>=<span class="string">"BIGINT"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">result</span> <span class="attr">column</span>=<span class="string">"userName"</span> <span class="attr">property</span>=<span class="string">"userName"</span> <span class="attr">jdbcType</span>=<span class="string">"VARCHAR"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">result</span> <span class="attr">column</span>=<span class="string">"passWord"</span> <span class="attr">property</span>=<span class="string">"passWord"</span> <span class="attr">jdbcType</span>=<span class="string">"VARCHAR"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">result</span> <span class="attr">column</span>=<span class="string">"user_sex"</span> <span class="attr">property</span>=<span class="string">"userSex"</span> <span class="attr">javaType</span>=<span class="string">"com.xxx.enums.UserSexEnum"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">result</span> <span class="attr">column</span>=<span class="string">"nick_name"</span> <span class="attr">property</span>=<span class="string">"nickName"</span> <span class="attr">jdbcType</span>=<span class="string">"VARCHAR"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">resultMap</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">sql</span> <span class="attr">id</span>=<span class="string">"Base_Column_List"</span> &gt;</span></span><br><span class="line">        id, userName, passWord, user_sex, nick_name</span><br><span class="line">    <span class="tag">&lt;/<span class="name">sql</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">"getAll"</span> <span class="attr">resultMap</span>=<span class="string">"BaseResultMap"</span>  &gt;</span></span><br><span class="line">       SELECT </span><br><span class="line">       <span class="tag">&lt;<span class="name">include</span> <span class="attr">refid</span>=<span class="string">"Base_Column_List"</span> /&gt;</span></span><br><span class="line">   FROM users</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">"getOne"</span> <span class="attr">parameterType</span>=<span class="string">"java.lang.Long"</span> <span class="attr">resultMap</span>=<span class="string">"BaseResultMap"</span> &gt;</span></span><br><span class="line">        SELECT </span><br><span class="line">       <span class="tag">&lt;<span class="name">include</span> <span class="attr">refid</span>=<span class="string">"Base_Column_List"</span> /&gt;</span></span><br><span class="line">   FROM users</span><br><span class="line">   WHERE id = #&#123;id&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">"insert"</span> <span class="attr">parameterType</span>=<span class="string">"com.xxx.entity.UserEntity"</span> &gt;</span></span><br><span class="line">       INSERT INTO </span><br><span class="line">       users</span><br><span class="line">       (userName,passWord,user_sex) </span><br><span class="line">       VALUES</span><br><span class="line">       (#&#123;userName&#125;, #&#123;passWord&#125;, #&#123;userSex&#125;)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">update</span> <span class="attr">id</span>=<span class="string">"update"</span> <span class="attr">parameterType</span>=<span class="string">"com.xxx.entity.UserEntity"</span> &gt;</span></span><br><span class="line">       UPDATE </span><br><span class="line">       users </span><br><span class="line">       SET </span><br><span class="line">       <span class="tag">&lt;<span class="name">if</span> <span class="attr">test</span>=<span class="string">"userName != null"</span>&gt;</span>userName = #&#123;userName&#125;,<span class="tag">&lt;/<span class="name">if</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">if</span> <span class="attr">test</span>=<span class="string">"passWord != null"</span>&gt;</span>passWord = #&#123;passWord&#125;,<span class="tag">&lt;/<span class="name">if</span>&gt;</span></span><br><span class="line">       nick_name = #&#123;nickName&#125;</span><br><span class="line">       WHERE </span><br><span class="line">       id = #&#123;id&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">update</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">delete</span> <span class="attr">id</span>=<span class="string">"delete"</span> <span class="attr">parameterType</span>=<span class="string">"java.lang.Long"</span> &gt;</span></span><br><span class="line">       DELETE FROM</span><br><span class="line">        users </span><br><span class="line">       WHERE </span><br><span class="line">        id =#&#123;id&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">delete</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mapper</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>使用</p></li></ol><h4 id="3-2定时任务"><a href="#3-2定时任务" class="headerlink" title="3.2定时任务"></a>3.2定时任务</h4><blockquote><p>参考<a href="http://www.ityouknow.com/springboot/2016/12/02/spring-boot-scheduler.html" target="_blank" rel="noopener">定时任务</a></p></blockquote><p>项目中经常会有定时执行的需求，在SpringBoot中可以通过注解快速实现。</p><ol><li><p>引入maven依赖</p><p>引入Spring Boot Starter 包即可</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>启动类添加注解启用定时</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="meta">@EnableScheduling</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">SpringApplication.run(Application<span class="class">.<span class="keyword">class</span>, <span class="title">args</span>)</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>创建定时任务实现类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SchedulerTask</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Scheduled</span>(cron=<span class="string">"*/6 * * * * ?"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doSomething1</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">//TO DO</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">     <span class="meta">@Scheduled</span>(fixedRate = <span class="number">6000</span>)</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doSomething2</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">//TO DO</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h4 id="3-3多数据源"><a href="#3-3多数据源" class="headerlink" title="3.3多数据源"></a>3.3多数据源</h4><blockquote><p>参考<a href="http://www.ityouknow.com/springboot/2016/11/25/spring-boot-multi-mybatis.html" target="_blank" rel="noopener">Mybatis多数据源最简解决方案</a></p></blockquote><p>项目中遇到应用需要连接多个数据库的需求，可以通过自定义配置类实现。</p><ol><li><p>配置文件中定义数据源</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据源1</span></span><br><span class="line"><span class="meta">api.datasource.driverClassName</span>=<span class="string">com.mysql.cj.jdbc.Driver</span></span><br><span class="line"><span class="meta">api.datasource.url</span>=<span class="string">jdbc:mysql://localhost:3306/test2?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=true</span></span><br><span class="line"><span class="meta">api.datasource.username</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">api.datasource.password</span>=<span class="string">root</span></span><br><span class="line"><span class="comment">#数据源2</span></span><br><span class="line"><span class="meta">secret.datasource.driverClassName</span>=<span class="string">com.mysql.cj.jdbc.Driver</span></span><br><span class="line"><span class="meta">secret.datasource.url</span>=<span class="string">jdbc:mysql://localhost:3306/test2?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=true</span></span><br><span class="line"><span class="meta">secret.datasource.username</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">secret.datasource.password</span>=<span class="string">root</span></span><br></pre></td></tr></table></figure></li><li><p>自定义实现配置类</p><p><strong>配置类中需要通过@Primary指定主库</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="comment">// baskPackages指定要扫描的包</span></span><br><span class="line"><span class="meta">@MapperScan</span>(basePackages = <span class="string">"com.xxx.apidao"</span>, sqlSessionTemplateRef  = <span class="string">"apiSqlSessionTemplate"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ApiDataSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"apiData"</span>)</span><br><span class="line">    <span class="comment">// 指定application.properteis中对应属性的前缀</span></span><br><span class="line">    <span class="meta">@ConfigurationProperties</span>(prefix = <span class="string">"api.datasource"</span>) </span><br><span class="line">    <span class="comment">// 指定一个数据源作为主库，其他的配置类不需要@Primary注解</span></span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataSource <span class="title">apiData</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> DataSourceBuilder.create().build();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"apiSqlSessionFactory"</span>)</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SqlSessionFactory <span class="title">apiSqlSessionFactory</span><span class="params">(@Qualifier(<span class="string">"apiData"</span>)</span> DataSource dataSource) <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        SqlSessionFactoryBean bean = <span class="keyword">new</span> SqlSessionFactoryBean();</span><br><span class="line">        bean.setDataSource(dataSource);</span><br><span class="line">        <span class="comment">// 指定xml所在的包，如果不同数据源的xml放在一个包，可以通过不同后缀名来区分</span></span><br><span class="line">        bean.setMapperLocations(<span class="keyword">new</span> PathMatchingResourcePatternResolver().getResources(<span class="string">"classpath:mapper/*ApiMapper.xml"</span>));</span><br><span class="line">        <span class="keyword">return</span> bean.getObject();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"apiTransactionManager"</span>)</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataSourceTransactionManager <span class="title">apiTransactionManager</span><span class="params">(@Qualifier(<span class="string">"apiData"</span>)</span> DataSource dataSource) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> DataSourceTransactionManager(dataSource);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"apiSqlSessionTemplate"</span>)</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlSessionTemplate <span class="title">apiSqlSessionTemplate</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">@Qualifier(<span class="string">"apiSqlSessionFactory"</span>)</span> SqlSessionFactory sqlSessionFactory) <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> SqlSessionTemplate(sqlSessionFactory);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>DAO层及xml文件</p><p>按下图所示组织包结构，将对应的dao层接口、xml文件按照配置类放到对应位置，xml可分文件夹存放，也可以在同一文件夹中通过不同前/后缀名称来区分。编写接口和xml时注意好对应关系即可。</p><p><img src="/2020/03/09/springboot/%E5%8C%85%E7%BB%93%E6%9E%84.png" alt="包结构"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Repository</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ApiUserDao</span> </span>&#123;</span><br><span class="line"><span class="comment">// 方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">"com.xxx.aipdao.ApiUserDao"</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- SQL --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mapper</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>使用</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> springboot </tag>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习(三) Spark SQL</title>
      <link href="/2020/03/08/Spark%E5%AD%A6%E4%B9%A0-%E4%B8%89-Spark-SQL/"/>
      <url>/2020/03/08/Spark%E5%AD%A6%E4%B9%A0-%E4%B8%89-Spark-SQL/</url>
      
        <content type="html"><![CDATA[<h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><p>Spark SQL是Spark的一个模块，用于处理结构化数据。</p><a id="more"></a><h2 id="Spark-SQL原理"><a href="#Spark-SQL原理" class="headerlink" title="Spark SQL原理"></a>Spark SQL原理</h2>]]></content>
      
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习(二) Spark Shuffle</title>
      <link href="/2020/03/08/Spark%E5%AD%A6%E4%B9%A0-%E4%BA%8C-Spark-Shuffle/"/>
      <url>/2020/03/08/Spark%E5%AD%A6%E4%B9%A0-%E4%BA%8C-Spark-Shuffle/</url>
      
        <content type="html"><![CDATA[<h1 id="Spark-Shuffle"><a href="#Spark-Shuffle" class="headerlink" title="Spark Shuffle"></a>Spark Shuffle</h1><h2 id="引起shuffle的算子"><a href="#引起shuffle的算子" class="headerlink" title="引起shuffle的算子"></a>引起shuffle的算子</h2><h2 id="Spark-Shuffle两个阶段"><a href="#Spark-Shuffle两个阶段" class="headerlink" title="Spark Shuffle两个阶段"></a>Spark Shuffle两个阶段</h2><p>每个stage的执行算子首先进行Shuffle Write，将中间结果持久化到磁盘，后继stage先进行Shuffle Read。</p><p>Shuffle过程中会发生数据分区，序列化反序列化，数据压缩，磁盘IO</p><a id="more"></a><h2 id="Shuffle调优"><a href="#Shuffle调优" class="headerlink" title="Shuffle调优"></a>Shuffle调优</h2><p>1.调整spill频率</p><ul><li>spark.shuffle.file.buffer</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习(一) Spark Core</title>
      <link href="/2020/03/08/Spark%E5%AD%A6%E4%B9%A0-%E4%B8%80-Spark-Core/"/>
      <url>/2020/03/08/Spark%E5%AD%A6%E4%B9%A0-%E4%B8%80-Spark-Core/</url>
      
        <content type="html"><![CDATA[<h1 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h1><h2 id="1-RDD"><a href="#1-RDD" class="headerlink" title="1.RDD"></a>1.RDD</h2><h3 id="1-1什么是RDD？"><a href="#1-1什么是RDD？" class="headerlink" title="1.1什么是RDD？"></a>1.1什么是RDD？</h3><p>RDD（Resilient Distributed Dataset）叫做<strong>弹性分布式数据集</strong>，<strong>是Spark中最基本的数据抽象</strong>，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p><a id="more"></a><h3 id="1-2RDD的属性和特点"><a href="#1-2RDD的属性和特点" class="headerlink" title="1.2RDD的属性和特点"></a>1.2RDD的属性和特点</h3><ul><li><p>只读</p><ul><li><p>通过HDFS或者其他持久化系统创建RDD</p></li><li><p>通过transformation将父RDD转化得到新的RDD</p></li><li><p>RDD上保存这前后之间的依赖关系</p></li></ul></li><li><p>Partition</p><ul><li>基本组成单位，RDD在逻辑上按照Partition分块</li><li>分布在各个节点上</li><li>分片数量决定并行计算的粒度</li><li>RDD中保存如何计算每一个分区的函数</li></ul></li><li><p>容错</p><ul><li>失败自动重建</li><li>如果发生部分分区数据丢失，可以通过依赖关系重新计算</li></ul></li></ul><h3 id="1-3创建RDD"><a href="#1-3创建RDD" class="headerlink" title="1.3创建RDD"></a>1.3创建RDD</h3><p>Spark Core提供了三种创建RDD的方式，如下：</p><ol><li><p>使用程序中的集合创建（主要用于测试）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; list = Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(list);</span><br></pre></td></tr></table></figure></li><li><p>使用本地文件创建（主要用于临时性处理有大量数据的文件）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SparkSession session = SparkSession.builder().master(<span class="string">"local"</span>).appName(<span class="string">"WordCountLocal"</span>).getOrCreate();</span><br><span class="line">JavaRDD&lt;String&gt; lines = session.read().textFile(<span class="string">"D:\\spark.txt"</span>).javaRDD();</span><br></pre></td></tr></table></figure></li><li><p>使用HDFS文件创建（生产环境的常用方式）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SparkSession spark = SparkSession.builder().appName(<span class="string">"WordCountCluster"</span>).getOrCreate();</span><br><span class="line">JavaRDD&lt;String&gt; lines = spark.read().textFile(<span class="string">"hdfs://h0:9000/spark.txt"</span>).javaRDD();</span><br></pre></td></tr></table></figure></li></ol><h3 id="1-4操作RDD"><a href="#1-4操作RDD" class="headerlink" title="1.4操作RDD"></a>1.4操作RDD</h3><p>Spark支持两种RDD操作：transformation和action。</p><h4 id="1-4-1transformation操作"><a href="#1-4-1transformation操作" class="headerlink" title="1.4.1transformation操作"></a>1.4.1transformation操作</h4><p>transformation算子会针对已有的RDD创建一个新的RDD。该类算子不会触发spark程序的执行，它们只是记录的对RDD进行的操作，只有当遇到action类型算子时，之前的所有transformation才会执行。</p><p>常用的transformation算子：</p><table><thead><tr><th>转换</th><th>说明</th></tr></thead><tbody><tr><td><strong>map</strong>(func)</td><td>返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</td></tr><tr><td><strong>filter</strong>(func)</td><td>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成</td></tr><tr><td><strong>flatMap</strong>(func)</td><td>类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）</td></tr><tr><td><strong>mapPartitions</strong>(func)</td><td>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]</td></tr><tr><td><strong>union</strong>(otherDataset)</td><td>对源RDD和参数RDD求并集后返回一个新的RDD</td></tr><tr><td><strong>intersection</strong>(otherDataset)</td><td>对源RDD和参数RDD求交集后返回一个新的RDD</td></tr><tr><td><strong>groupByKey</strong>([numTasks])</td><td>在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD</td></tr><tr><td><strong>reduceByKey</strong>(func, [numTasks])</td><td>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置</td></tr><tr><td><strong>sortByKey</strong>([ascending], [numTasks])</td><td>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</td></tr><tr><td><strong>join</strong>(otherDataset, [numTasks])</td><td>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD 相当于内连接（求交集）</td></tr><tr><td><strong>cogroup</strong>(otherDataset, [numTasks])</td><td>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD</W></V></td></tr></tbody></table><h4 id="1-4-2action操作"><a href="#1-4-2action操作" class="headerlink" title="1.4.2action操作"></a>1.4.2action操作</h4><p>action算子触发代码的执行。</p><p>常用的action算子：</p><table><thead><tr><th>动作</th><th>说明</th></tr></thead><tbody><tr><td><strong>reduce</strong>(<em>func</em>)</td><td>通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的</td></tr><tr><td><strong>collect</strong>()</td><td>在驱动程序中，以数组的形式返回数据集的所有元素</td></tr><tr><td><strong>count</strong>()</td><td>返回RDD的元素个数</td></tr><tr><td><strong>first</strong>()</td><td>返回RDD的第一个元素（类似于take(1)）</td></tr><tr><td><strong>take</strong>(<em>n</em>)</td><td>返回一个由数据集的前n个元素组成的数组</td></tr><tr><td><strong>saveAsTextFile</strong>(<em>path</em>)</td><td>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</td></tr><tr><td><strong>foreach</strong>(<em>func</em>)</td><td>在数据集的每一个元素上，运行函数func进行更新。</td></tr><tr><td><strong>countByKey</strong>()</td><td>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</td></tr></tbody></table><h4 id="1-4-3RDD持久化"><a href="#1-4-3RDD持久化" class="headerlink" title="1.4.3RDD持久化"></a>1.4.3RDD持久化</h4><p>调用cache()或者persist()方法即可。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = spark.read.textFile(<span class="string">"hdfs://h0:9000/spark.txt"</span>).persist()</span><br></pre></td></tr></table></figure><h2 id="2-共享变量"><a href="#2-共享变量" class="headerlink" title="2.共享变量"></a>2.共享变量</h2><p>Spark提供了两种共享变量：Broadcast Variable（广播变量）和Accumulator（累加器）。</p><h3 id="2-1Broadcast-Variable（广播变量）"><a href="#2-1Broadcast-Variable（广播变量）" class="headerlink" title="2.1Broadcast Variable（广播变量）"></a>2.1Broadcast Variable（广播变量）</h3><p>广播变量会将使用到的变量，仅仅为每个executor拷贝一份，这个executor启动的task会共享这个变量，从而减少网络传输以及内存消耗。广播变量是只读的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//声明一个广播变量</span></span><br><span class="line"><span class="keyword">val</span> factor = <span class="number">3</span></span><br><span class="line"><span class="keyword">val</span> broadcastVars = sc.broadcast(factor)</span><br><span class="line"><span class="comment">//获取一个广播变量的值</span></span><br><span class="line"><span class="keyword">val</span> value = broadcastVars.value</span><br></pre></td></tr></table></figure><p><img src="/2020/03/08/Spark%E5%AD%A6%E4%B9%A0-%E4%B8%80-Spark-Core/%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.png" alt="广播变量"></p><h3 id="2-2Accumulator（累加器）"><a href="#2-2Accumulator（累加器）" class="headerlink" title="2.2Accumulator（累加器）"></a>2.2Accumulator（累加器）</h3><p>在spark应用程序中，我们经常会有这样的需求，如异常监控、调试，记录符合某特性的数据条目数等，这都需要使用到计数器，如果一个变量不被声明为一个累加器，那么它在改变时不会在driver端进行全局汇总，即在分布式运行时每个task运行的只是一个变量的副本，并不能改变原始变量的值。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//声明一个累加器</span></span><br><span class="line"><span class="keyword">val</span> accumulator = sc.accumulator(<span class="number">0</span>)</span><br><span class="line"><span class="comment">//获取累加器的值</span></span><br><span class="line"><span class="keyword">val</span> value = accumulator.value</span><br></pre></td></tr></table></figure><h2 id="3-宽窄依赖"><a href="#3-宽窄依赖" class="headerlink" title="3.宽窄依赖"></a>3.宽窄依赖</h2><p>由于RDD是粗粒度的操作数据集，每个transformation都会生成一个新的RDD，所以RDD之间就会形成类似流水线的前后依赖关系，这种依赖关系分为两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。</p><p><img src="/2020/03/08/Spark%E5%AD%A6%E4%B9%A0-%E4%B8%80-Spark-Core/%E5%AE%BD%E7%AA%84%E4%BE%9D%E8%B5%96.png" alt="宽窄依赖图示"></p><p>窄依赖：是指每个父RDD的一个Partition最多被子RDD的一个Partition所使用，例如map、filter、union等操作都会产生窄依赖；</p><p>宽依赖：是指一个父RDD的Partition会被多个子RDD的Partition所使用，例如groupByKey、reduceByKey、sortByKey等操作都会产生宽依赖。</p><p><strong>窄依赖无数据shuffle，宽依赖存在数据shuffle</strong></p><h3 id="3-1依赖关系下的stage划分"><a href="#3-1依赖关系下的stage划分" class="headerlink" title="3.1依赖关系下的stage划分"></a>3.1依赖关系下的stage划分</h3><p><img src="/2020/03/08/Spark%E5%AD%A6%E4%B9%A0-%E4%B8%80-Spark-Core/stage%E5%88%92%E5%88%86.png" alt="stage划分"></p><p>在spark中，会根据RDD之间的依赖关系将DAG（有向无环图）划分为不同的stage，对于窄依赖，由于Partition依赖关系的确定性，Partition的转换处理就可以在同一个线程里完成，窄依赖就被划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。</p><p>因此spark划分stage的思路：<strong>从后往前推，以shuffle操作作为边界，遇到一个宽依赖就分一个stage。</strong></p><h2 id="4-Spark内存模型"><a href="#4-Spark内存模型" class="headerlink" title="4.Spark内存模型"></a>4.Spark内存模型</h2><h3 id="4-1yarn资源调度过程"><a href="#4-1yarn资源调度过程" class="headerlink" title="4.1yarn资源调度过程"></a>4.1yarn资源调度过程</h3><p><img src="/2020/03/08/Spark%E5%AD%A6%E4%B9%A0-%E4%B8%80-Spark-Core/yarn%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B.png" alt="yarn调度过程"></p><blockquote><p>说明</p><p>1.Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend；</p><p>2.ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派；</p><p>3.Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）；</p><p>4.一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task；</p><p>5.Client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；</p><p>6.应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。</p></blockquote><h3 id="4-2Spark内存结构"><a href="#4-2Spark内存结构" class="headerlink" title="4.2Spark内存结构"></a>4.2Spark内存结构</h3><p><img src="/2020/03/08/Spark%E5%AD%A6%E4%B9%A0-%E4%B8%80-Spark-Core/spark%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84.png" alt="spark内存结构"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
